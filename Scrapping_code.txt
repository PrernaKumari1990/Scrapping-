
!pip install requests --upgrade

import requests

topics_url='https://github.com/topics'

response= requests.get(topics_url)

response.status_code

len(response.text)

page_contents = response.text

page_contents[:1000]

with open('webpage.html', 'w', encoding='utf-8') as f:
    f.write(page_contents)

!pip install beautifulsoup4 --upgrade

from bs4 import BeautifulSoup

doc = BeautifulSoup(page_contents, 'html.parser')

type(doc)

p_tags = doc.find_all('p')

len(p_tags)

p_tags[:5]

selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'
topic_title_tags = doc.find_all('p',{'class':selection_class})

len(topic_title_tags)

topic_title_tags

topic_title_tags[:5]

desc_selector = 'f5 color-fg-muted mb-0 mt-1'
topic_desc_tags = doc.find_all('p',{'class':desc_selector})
         
topic_desc_tags[:5]

topic_link_tags= doc.find_all('a',{'class':'no-underline flex-grow-0'})

len(topic_link_tags)

topic_link_tags[0]

topic_link_tags[0]['href']

topic0_url = "https://github.com" + topic_link_tags[0]['href']
print(topic0_url)

topic_title_tags[0].text

topic_titles= []

for tag in topic_title_tags:
    topic_titles.append(tag.text)

print(topic_titles)

topic_desc = []

for tag in topic_desc_tags:
    topic_desc.append(tag.text.strip())

print(topic_desc)

topic_urls = []

for tag in topic_link_tags:
    topic_urls.append(tag['href'])
    
topic_urls

topic_urls = []
base_url = 'https://github.com'

for tag in topic_link_tags:
    topic_urls.append(base_url + tag['href'])
    
topic_urls

!pip install pandas

import pandas as pd

topics_dict={
    'title': topic_titles,
    'description': topic_desc,
    'url': topic_urls
}

topics_df= pd.DataFrame(topics_dict)

topics_df

topics_df.to_csv('topics.csv')

topic_page_url= topic_urls[0]

topic_page_url

response = requests.get(topic_page_url)

response.status_code

len(response.text)

topic_doc = BeautifulSoup(response.text,'html.parser')

h3_selection_class = 'f3 color-fg-muted text-normal lh-condensed'
repo_tags = topic_doc.find_all('h3',{'class': h3_selection_class})

len(repo_tags)

a_tags = repo_tags[0].find_all('a')

a_tags

a_tags[0].text.strip()

a_tags[1]

a_tags[1].text.strip()

a_tags[1]['href']

base_url

base_url = 'https://github.com'
repo_url = base_url + a_tags[1]['href']
print(repo_url)

star_tags = topic_doc.find_all('a', {'class': 'tooltipped tooltipped-s btn-sm btn BtnGroup-item color-bg-default'})

len(star_tags)

star_tags[0].text.strip()

def parse_star_count(stars_str):
    stars_str = stars_str.replace('Star', '').strip()
    
    if stars_str[-1] == 'k':
        return int(float(stars_str[:-1]) * 1000)
    
    return int(stars_str)


parse_star_count(star_tags[0].text.strip())

def get_repo_info(h3_tag,star_tag):
    #returns all the required info about a repository
    a_tags = h3_tag.find_all('a')
    username = a_tags[0].text.strip()
    repo_name = a_tags[1].text.strip()
    repo_url = base_url + a_tags[1] ['href']
    stars = parse_star_count(star_tags[0].text.strip())
    return username, repo_name,stars,repo_url
    
get_repo_info(repo_tags[0],star_tags[0])

topic_repos_dict = {
    'username' : [],
    'repo_name': [],
    'stars': [],
    'repo_url': []
}

for i in range(len(repo_tags)):
    repo_info = get_repo_info(repo_tags[i],star_tags[i])
    topic_repos_dict['username'].append(repo_info[0])
    topic_repos_dict['repo_name'].append(repo_info[1])
    topic_repos_dict['stars'].append(repo_info[2])
    topic_repos_dict['repo_url'].append(repo_info[3])
    
import os
def get_topic_page(topic_url):
    # Download the page
    response = requests.get(topic_url)
    # Check successful response
    if response.status_code != 200:
        raise Exception('Failed to load page {}'.format(topic_url))
    # Parse Beautiful Soup
    topic_doc = BeautifulSoup(response.text, 'html.parser')
    return topic_doc

    
def get_repo_info(h3_tag,star_tag):
    a_tags = h3_tag.find_all('a')
    username = a_tags[0].text.strip()
    repo_name = a_tags[1].text.strip()
    repo_url = base_url + a_tags[1] ['href']
    stars = parse_star_count(star_tags[0].text.strip())
    return username, repo_name,stars,repo_url

def get_topic_repos(topic_doc):
     #Get h3 tags containing repo title, repo url, username
        h3_selection_class = 'f3 color-fg-muted text-normal lh-condensed'
        repo_tags = topic_doc.find_all('h3',{'class': h3_selection_class})  
        #Get star tags
        star_tags = topic_doc.find_all('a', {'class': 'tooltipped tooltipped-s btn-sm btn BtnGroup-item color-bg-default'})
        
        topic_repos_dict = {
            'username' : [],
            'repo_name': [],
             'stars': [],
             'repo_url': []
      }
    
     #Get repo info
        for i in range(len(repo_tags)):
            repo_info = get_repo_info(repo_tags[i],star_tags[i])
            topic_repos_dict['username'].append(repo_info[0])
            topic_repos_dict['repo_name'].append(repo_info[1])
            topic_repos_dict['stars'].append(repo_info[2])
            topic_repos_dict['repo_url'].append(repo_info[3])
            
        return pd.DataFrame(topic_repos_dict)
    
def scrape_topic(topic_url,path):
    if os.path.exists(path):
        print("The file {} already exists. Skipping....".format(path))
        return
    topic_df = get_topic_repos(get_topic_page(topic_url))
    topic_df.to_csv(path,index=None)
            

        
 
def get_topic_titles(doc):
    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'
    topic_title_tags = doc.find_all('p',{'class':selection_class})
    topic_titles = []
    for tag in topic_title_tags:
        topic_titles.append(tag.text)
    return topic_titles

def get_topic_descs(doc):
    desc_selector = 'f5 color-fg-muted mb-0 mt-1'
    topic_desc_tags = doc.find_all('p',{'class':desc_selector})
    topic_descs = []
    for tag in topic_desc_tags:
        topic_descs.append(tag.text.strip())
    return topic_descs

def get_topic_urls(doc):
    topic_link_tags = doc.find_all('a',{'class':'no-underline flex-grow-0'})
    topic_urls = []
    base_url = 'https://github.com'
    for tag in topic_link_tags:
        topic_urls.append(base_url + tag['href'])
    return topic_urls

def scrape_topics():
    topics_url = 'https://github.com/topics'
    response = requests.get(topics_url)
    if response.status_code != 200:
        raise Exception('Failed to load page {}'.format(topics_url))
    
    doc = BeautifulSoup(response.text, 'html.parser')
    
    topics_dict ={
        'title': get_topic_titles(doc),
        'description': get_topic_descs(doc),
        'url': get_topic_urls(doc)
    }
    
    return pd.DataFrame(topics_dict)


    

import os

help(os.makedirs)

def scrape_topics_repos():
    print('Scraping list of topics')
    topics_df = scrape_topics()
    
    os.makedirs('data',exist_ok=True)
    for index,row in topics_df.iterrows():
        print('Scrapping top repositories for "{}"' .format(row['title']))
        scrape_topic(row['url'],'data/{}.csv' .format(row['title']))

scrape_topics_repos()